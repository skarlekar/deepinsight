# CLAUDE.md

This file provides comprehensive guidance to Claude Code (claude.ai/code) when working with the DeepInsight project repository.

## Project Overview

**DeepInsight** is an enterprise-grade LLM-based agentic AI system for extracting structured information from unstructured documents using customizable ontologies. The system transforms documents into knowledge graphs compatible with Neo4j and AWS Neptune databases.

### Key Features
- **AI-Powered Ontology Creation**: Automatically generate ontologies from documents using Claude Sonnet 4
- **Interactive Graph Visualization**: Real-time network graph visualization with vis.js
- **Multi-Database Support**: Export to Neo4j and AWS Neptune formats
- **Enterprise Security**: OWASP-compliant security with comprehensive audit logging
- **Real-time Processing**: WebSocket-based progress tracking and notifications
- **Pluggable LLM Architecture**: Support for Claude, GPT-4, and other LangChain-compatible LLMs

## System Architecture

### Simplified MVP Architecture
Streamlined two-tier architecture optimized for <5 users:

```
React Frontend ↔ FastAPI Backend ↔ SQLite + File Storage
     ↓                    ↓              ↓
   vis.js             LangGraph      JSON Files
 Visualization         Agents       (Documents)
```

**Key Simplifications for MVP:**
- **SQLite Database**: Single-file database instead of PostgreSQL
- **File-Based Storage**: JSON files for documents and ontologies
- **Simple Authentication**: JWT without complex RBAC
- **Direct LLM Integration**: Claude Sonnet 4 only (no provider abstraction)
- **Minimal Monitoring**: Basic logging without enterprise metrics

The following is the detailed specification for the project:

<Specification>
The ontology itself is a JSON structure that consists of  tuples such as, 'subject', 'relationship' and 'object' forming "subject-predicate-object" triple.  The 'subject' and 'object' are entity types and 'relationship' describes the type of relationship between the 'subject' and 'object'. The results should be supplied in JSON format as an array of JSON objects. For example, the following JSON object: 

```json
{
  "subject": {
    "entity_type": "Person",
    "type_variations": [
      "Employee",
      "Individual"
    ],
    "primitive_type": "string"
  },
  "relationship": {
    "relationship_type": "works_for",
    "type_variations": [
      "is_employed_by",
      "employed_at",
      "works_at"
    ]
  },
  "object": {
    "entity_type": "Organization",
    "type_variations": [
      "Company",
      "Employer"
    ],
    "primitive_type": "string"
  }
}
```
represents an employment relationship of a person with an organization. 

The following JSON object represents the salary earned by the person:

```json
{
  "subject": {
    "entity_type": "Person",
    "type_variations": [
      "Employee",
      "Individual"
    ],
    "primitive_type": "string"
  },
  "relationship": {
    "relationship_type": "earns",
    "type_variations": [
      "salaried_at",
      "is_paid"
    ]
  },
  "object": {
    "entity_type": "Salary",
    "type_variations": [
      "Remuneration",
      "Compensation"
    ],
    "primitive_type": "float"
  }
}
```

The final ontology will consist of a list of such ontologies as JSON array. For example:

```json
[
  {
    "subject": {
      "entity_type": "Person",
      "type_variations": [
        "Employee",
        "Individual"
      ],
      "primitive_type": "string"
    },
    "relationship": {
      "relationship_type": "works_for",
      "type_variations": [
        "is_employed_by",
        "employed_at",
        "works_at"
      ]
    },
    "object": {
      "entity_type": "Organization",
      "type_variations": [
        "Company",
        "Employer"
      ],
      "primitive_type": "string"
    }
  },
  {
    "subject": {
      "entity_type": "Person",
      "type_variations": [
        "Employee",
        "Individual"
      ],
      "primitive_type": "string"
    },
    "relationship": {
      "relationship_type": "earns",
      "type_variations": [
        "salaried_at",
        "is_paid"
      ]
    },
    "object": {
      "entity_type": "Salary",
      "type_variations": [
        "Remuneration",
        "Compensation"
      ],
      "primitive_type": "float"
    }
  }
]
```

The ontology itself will be generated by the LLM and presented to the user as described in the ‘Ontology Creation’ section. The user can then add, edit or delete the rows in the ontology. When the user is satisified by the ontology, he or she can then use the ontology to extract actual entities and relationships from the document that will be used to build the graph as described in the ‘Data Extraction using Ontology’ section.  

<OntologyCreation>
The given document should first be chunked with overlapping chunks. For each chunk, the application should extract entities and relationships. Each item in the list of entities and relations in the ontology should refer to only one entity or relation in the singular form without any adjective prefixes or explanations. It is very important that there are no duplicate entities or relations in the final list of entities and relations.  The application should be thorough in its extraction of entities and relations and should not miss any. All the extracted entities and relationships should then be deduplicated so that only unique tuples are returned as a result of the ontology creation.

Once the ontology is created, the user interface should provide a method to download the ontology and upload previously downloaded ontology. 
</OntologyCreation>

<DataExtractionUsingOntology>
When the user is ready to extract the actual data from the document using the ontology, the application should then chunk the given document into chunks with overlapping chunks and use the ontology to guide the extraction of the data from the document. The extraction of actual entities and relationship should also include the location of the entity or relationship in the original document so that the location is available as an attribute of the entity or relationship. 

As the LLM will make multiple passes on each chunk to extract the nodes and relationship, it is important to deduplicate the nodes after every pass to ensure the resulting data does not have duplicate nodes.

The output of the extracted data should be available in two formats. One set for loading the data in AWS Neptune database as neptune_edges.csv and neptune_vertices.csv and the second set for loading the data in Neo4j AuraDB as neo4j_nodes.csv and neo4j_relationships.csv
</DataExtractionUsingOntology>

<TechnicalSpecifications>

<Frontend>
**Technology Stack (Definitive Choices):**
- **Framework**: React 18.2+ with TypeScript 5.0+
- **UI Library**: Material-UI (MUI) v5.14+ with custom theme
- **State Management**: Zustand + TanStack Query v4+ (no Redux)
- **Visualization**: vis.js Network 9.1+ for interactive graph visualization
- **Build Tool**: Vite 4+ with ESBuild
- **Testing**: Vitest + React Testing Library + Playwright E2E
- **WebSocket**: Native WebSocket API with auto-reconnection logic

**Key Components:**
- **Document Upload Manager**: Drag-and-drop with progress tracking and file validation
- **Interactive Ontology Editor**: CRUD operations with real-time validation
- **Network Graph Visualization**: High-performance vis.js with 10k+ node support
- **Database Connection Manager**: Neo4j and Neptune connection testing
- **Real-time Progress Dashboard**: WebSocket-based status updates
- **Error Boundary Components**: React error boundaries with recovery strategies
- **Mobile-responsive Design**: Progressive web app with offline support

**Performance Requirements:**
- **Initial Load**: < 2 seconds for application bootstrap
- **Graph Rendering**: < 3 seconds for 1000+ nodes, < 10 seconds for 10k+ nodes
- **API Response**: < 200ms for standard operations, < 500ms for complex queries
- **Mobile Performance**: Optimized touch interactions and responsive layouts
</Frontend>

<Backend>
**Enhanced Architecture with Enterprise Patterns:**
- **Framework**: FastAPI 0.104+ with Python 3.11+
- **Agent System**: LangGraph with sophisticated agent workflows
- **LLM Integration**: Pluggable provider architecture with circuit breakers
- **Database**: PostgreSQL 15+ primary, Redis 7+ for caching
- **Background Tasks**: Celery with Redis broker and advanced queue management
- **Monitoring**: Prometheus metrics, structured logging, health checks
- **Security**: Multi-layer validation, OWASP compliance, audit logging

**LLM Provider Architecture:**
```python
class BaseLLMProvider(ABC):
    @abstractmethod
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> LLMResponse:
        pass

class AnthropicProvider(BaseLLMProvider):
    # Claude Sonnet 4 implementation with circuit breaker
    
class OpenAIProvider(BaseLLMProvider):
    # GPT-4 implementation with fallback logic
```

**Key Services:**
- **Document Processing Service**: Async chunking with overlap management
- **Ontology Management Service**: CRUD operations with versioning
- **Data Extraction Service**: LangGraph agent orchestration
- **Database Export Service**: Multi-format export (Neo4j/Neptune)
- **LLM Integration Service**: Provider abstraction with failover
- **Authentication & Authorization Service**: JWT with RBAC
- **WebSocket Communication Service**: Real-time progress and notifications

**Circuit Breaker Implementation:**
- **LLM Services**: 5 failures → 30s open → half-open test
- **Database**: Connection pooling with automatic retry
- **File Operations**: Exponential backoff retry (3 attempts)

**Configuration Management:**
- **Environment-specific settings**: Development, staging, production
- **Nested configuration structure**: Database, LLM, cache, security settings
- **Validation**: Pydantic-based configuration validation
- **Secret management**: Environment variables with required key validation

**Default LLM**: Claude Sonnet 4 (claude-sonnet-4-20250514)
**API Key Requirements**: ANTHROPIC_API_KEY (required), OPENAI_API_KEY (optional fallback)
**Error Handling**: Server fails to start if required API keys are missing
</Backend>

<Database>
**Simplified MVP Database Strategy:**
- **Primary Database**: SQLite with simple sync operations
- **File Storage**: Local filesystem with basic security
- **Export Targets**: Neo4j and AWS Neptune (CSV format export)
- **Data Persistence**: JSON files for documents and ontologies

**Storage Architecture:**
- **SQLite**: User accounts, processing status, metadata
- **File System**: Uploaded documents, generated ontologies, extraction results
- **No Caching**: Direct database/file access for simplicity
- **Backup**: Simple file-based backup of SQLite + data directory
</Database>

<Security>
**Simplified MVP Security Framework:**
- **Authentication**: Basic JWT tokens for session management
- **Authorization**: Simple user-based access (no complex roles)
- **Data Protection**: HTTPS for transport, basic file permissions
- **Input Validation**: Pydantic validation with basic sanitization
- **Rate Limiting**: Simple per-user limits using in-memory counters
- **Audit Logging**: Basic access logging to files
- **Basic Security**: Essential protections without enterprise complexity
</Security>
</TechnicalSpecifications>
</Specification>

## Implementation Requirements

Based on this comprehensive specification, implement the DeepInsight system following these detailed requirements:

### 1. System Design & Architecture Implementation
**Create a simplified two-tier architecture** (React frontend ↔ FastAPI backend):

**Key Architecture Requirements:**
- **Eliminate Node.js middleware layer** - Direct React-to-FastAPI communication only
- **Two-tier system design** with clear separation of concerns
- **WebSocket integration** for real-time progress tracking and notifications
- **Circuit breaker pattern** implementation for external API resilience
- **Multi-level caching strategy** (L1: in-memory, L2: Redis distributed cache)
- **Horizontal scaling support** with stateless services and load balancing
- **Comprehensive monitoring** with Prometheus metrics and structured logging

**Required Documentation:**
- Complete system architecture with component diagrams
- Data flow specifications for ontology creation and extraction workflows
- Database strategy covering PostgreSQL primary, Redis cache, and graph DB exports
- Performance optimization and scaling strategies
- Error recovery and disaster recovery procedures

### 2. Frontend Implementation Requirements
**Technology Stack (Non-Negotiable):**
- **React 18.2+** with TypeScript 5.0+ (mandatory)
- **Material-UI (MUI) v5.14+** with custom theming (no other UI library)
- **Zustand + TanStack Query v4+** for state management (no Redux)
- **vis.js Network 9.1+** for graph visualization (not pyvis.network, not react-force-graph)
- **Vite 4+** with ESBuild for build tooling
- **Native WebSocket API** with auto-reconnection logic

**Core Components to Build:**
- **Document Upload Manager**: Drag-and-drop interface with progress tracking, file validation, and error handling
- **Interactive Ontology Editor**: Full CRUD operations with real-time validation and export/import functionality
- **Network Graph Visualization**: High-performance vis.js implementation supporting 10,000+ nodes with clustering and filtering
- **Database Connection Manager**: Neo4j and Neptune connection testing with status indicators
- **Real-time Progress Dashboard**: WebSocket-based live updates for document processing status
- **Error Boundary Components**: React error boundaries with user-friendly recovery strategies
- **Mobile-responsive Design**: Progressive web app with touch-optimized interactions

**Performance Requirements:**
- Initial application load: < 2 seconds
- Graph rendering: < 3 seconds for 1000+ nodes, < 10 seconds for 10,000+ nodes
- API response times: < 200ms standard operations, < 500ms complex queries
- Mobile performance optimization with responsive layouts

### 3. Backend Implementation Requirements (MVP Simplified)
**Technology Stack:**
- **FastAPI 0.104+** with Python 3.11+
- **LangGraph** for agent workflows
- **SQLite** as primary database with simple sync operations
- **Pydantic v2** for data validation
- **No Redis/Celery**: Synchronous processing for simplicity

**LLM Integration (Simplified):**
```python
# Simple direct integration - no provider abstraction needed
from anthropic import Anthropic

class LLMService:
    def __init__(self):
        self.client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    def generate(self, messages: List[Dict[str, str]]) -> str:
        # Direct Claude Sonnet 4 integration
        pass
```

**Required Services (Simplified):**
- **Document Processing Service**: Basic chunking with text extraction
- **Ontology Management Service**: Simple CRUD operations with JSON storage
- **Data Extraction Service**: Basic LangGraph workflow
- **Database Export Service**: Neo4j/Neptune CSV generation
- **Authentication Service**: Simple JWT implementation
- **File Management Service**: Basic file upload/download

**No Circuit Breakers or Complex Error Handling**: Simple retry logic only

### 4. Database Implementation Requirements (MVP Simplified)
**Simple Storage Strategy:**
- **SQLite**: Single-file database for user data and metadata
- **File System**: Local directory structure for documents and results
- **Export Targets**: Generate CSV formats for Neo4j and AWS Neptune

**SQLite Tables (Minimal):**
- **users**: id, username, password_hash, created_at
- **documents**: id, user_id, filename, file_path, status, created_at
- **ontologies**: id, user_id, document_id, name, file_path, created_at
- **extractions**: id, user_id, ontology_id, document_id, result_path, status, created_at

**File Structure:**
```
data/
├── database.sqlite
├── documents/
│   └── user_{id}/
│       ├── uploads/
│       ├── ontologies/
│       └── extractions/
```

### 5. Security Implementation Requirements (MVP Simplified)
**Basic Security Framework:**
- **Authentication**: Simple JWT tokens (no refresh rotation needed for MVP)
- **Authorization**: User-based access only (no complex roles)
- **Data Protection**: HTTPS for transport, basic file system permissions
- **Input Validation**: Pydantic validation with basic XSS prevention
- **Rate Limiting**: Simple in-memory counters per user
- **Logging**: Basic access logs to files
- **File Security**: Basic file type validation and size limits

### 6. Testing Implementation Requirements (MVP Simplified)
**Basic Test Coverage:**
- **Unit Tests**: 60%+ backend coverage, 50%+ frontend coverage
- **Integration Tests**: Basic API endpoint testing
- **Manual Testing**: Critical user journeys tested manually
- **LLM Mocking**: Simple response mocking for development

**Simple Test Infrastructure:**
- **Frontend**: Vitest + React Testing Library (basic tests only)
- **Backend**: pytest with simple fixtures
- **No CI/CD**: Manual testing for MVP phase

### 7. DevOps Implementation Requirements (MVP Simplified)
**Simple Deployment:**
- **Development**: Docker Compose for local development
- **Production**: Single server deployment (Heroku or DigitalOcean)

**Basic Deployment Features:**
- **Simple deployment**: Git-based deployment to single server
- **Basic monitoring**: Application logs and simple health checks
- **File backup**: Simple backup of SQLite + data directory
- **No complex orchestration**: Single container deployment

### 8. Performance & Scalability Requirements (MVP Simplified)
**Basic Performance Targets:**
- Document processing: 1-10 minutes per document (acceptable for MVP)
- API response times: < 1 second for basic operations
- Concurrent users: 5 users maximum (MVP scope)
- Graph visualization: Handle up to 1000 nodes efficiently
- Memory usage: Basic optimization, no complex management needed

**No Scaling Needed:**
- Single server deployment sufficient for <5 users
- SQLite can handle the load for MVP
- No load balancing or clustering required

## Implementation Guidelines (MVP Simplified)

### Phase 1: Basic MVP (Weeks 1-2)
1. **Simple Authentication**: User login/register with JWT
2. **Document Upload**: Basic file upload to local storage
3. **SQLite Setup**: Simple database with user and document tables
4. **Basic Frontend**: React setup with MUI, basic routing

### Phase 2: Core Features (Weeks 3-4)
1. **LangGraph Integration**: Basic ontology creation workflow
2. **Claude Integration**: Direct API integration for text processing
3. **Simple Ontology Editor**: Basic CRUD for ontology JSON files
4. **File Processing**: Text extraction from PDF/DOCX/TXT

### Phase 3: Graph & Export (Weeks 5-6)
1. **vis.js Integration**: Basic graph visualization
2. **Data Extraction**: Simple LangGraph workflow for entity extraction
3. **CSV Export**: Generate Neo4j and Neptune compatible files
4. **Basic Testing**: Essential functionality tests

### Phase 4: Polish & Deploy (Weeks 7-8)
1. **UI Polish**: Improve user experience and error handling
2. **Simple Deployment**: Deploy to Heroku or similar platform
3. **Basic Documentation**: User guide and API documentation
4. **Bug Fixes**: Address any critical issues

## Quality Gates & Success Criteria (MVP)

**Technical Requirements:**
- Basic security implemented (authentication, input validation)
- Core functionality works for 5 users
- Basic test coverage (50% backend, minimal frontend)
- Manual testing of critical user journeys
- Simple deployment to cloud platform

**Business Requirements:**
- Successfully upload and process PDF/DOCX/TXT documents
- Generate basic ontologies from documents using Claude
- Edit and save ontologies as JSON files
- Extract entities using ontologies with LangGraph
- Export data to Neo4j and Neptune CSV formats
- Simple deployment accessible over internet

This simplified specification ensures a working MVP can be delivered quickly for <5 users without enterprise complexity.


